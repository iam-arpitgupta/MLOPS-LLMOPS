{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# This Python 3 environment comes with many helpful analytics libraries installed\n",
    "# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n",
    "# For example, here's several helpful packages to load\n",
    "\n",
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "\n",
    "# Input data files are available in the read-only \"../input/\" directory\n",
    "# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n",
    "\n",
    "import os\n",
    "for dirname, _, filenames in os.walk('/kaggle/input'):\n",
    "    for filename in filenames:\n",
    "        print(os.path.join(dirname, filename))\n",
    "\n",
    "# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n",
    "# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import numpy as np \n",
    "import pandas as pd \n",
    "\n",
    "import matplotlib.pyplot as plt \n",
    "\n",
    "import seaborn as sns \n",
    "\n",
    "df = pd.read_csv(\"/kaggle/input/crop-recommendation-dataset/Crop_recommendation.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "df.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "df.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "df.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "df.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "df['label'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "df['label'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "df.drop('label', axis=1).hist(figsize=(16,12), bins=20, color='skyblue', edgecolor='black')\n",
    "plt.suptitle(\"Feature Distributions\", fontsize=16)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10,8))\n",
    "sns.heatmap(df.drop('label', axis=1).corr(), annot=True, cmap=\"YlGnBu\", fmt=\".2f\")\n",
    "plt.title(\"Feature Correlation Heatmap\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# feature engineering \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import LabelEncoder , MinMaxScaler , StandardScaler \n",
    "from sklearn.model_selection import train_test_split \n",
    "# from sklearn.model_training import train_test_split\n",
    "X = df.drop('label' , axis = 1)\n",
    "y = df['label']\n",
    "\n",
    "labelEncoder = LabelEncoder()\n",
    "y_encoded = labelEncoder.fit_transform(y)\n",
    "\n",
    "\n",
    "X_train , X_test , y_train, y_test = train_test_split(X , y_encoded , test_size = 0.2 , random_state = 42)\n",
    "# normalize the feature \n",
    "scaler = MinMaxScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model training \n",
    "decision Tree \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "from sklearn.metrics import accuracy_score\n",
    "DecisionTree = DecisionTreeClassifier(criterion=\"entropy\",random_state=2,max_depth=5)\n",
    "\n",
    "DecisionTree.fit(X_train_scaled,y_train)\n",
    "\n",
    "predict = DecisionTree.predict(X_test_scaled)\n",
    "\n",
    "x = accuracy_score(y_test, predict)\n",
    "print(x)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# hyper-params tuning \n",
    "from sklearn.model_selection import cross_val_score \n",
    "\n",
    "score = cross_val_score(DecisionTree, features , target,cv = 5)\n",
    "\n",
    "score "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "from sklearn.naive_bayes import GaussianNB\n",
    "\n",
    "naiveBayes = GaussianNB()\n",
    "\n",
    "\n",
    "NaiveBayes.fit(X_train_scaled ,y_train)\n",
    "\n",
    "predicted_values = NaiveBayes.predict(X_scaled_test)\n",
    "x = metrics.accuracy_score(y_test, predicted_values)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    " # Cross validation score (NaiveBayes)\n",
    "score = cross_val_score(NaiveBayes,features,target,cv=5)\n",
    "score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# logistic regression\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "LogReg = LogisticRegression(random_state=2)\n",
    "\n",
    "LogReg.fit(X_train_scaled ,y_train)\n",
    "\n",
    "predicted_values = LogReg.predict(X_scaled_test)\n",
    "\n",
    "x = metrics.accuracy_score(y_test, predicted_values)\n",
    "\n",
    "# Cross validation score (Logistic Regression)\n",
    "score = cross_val_score(LogReg,features,target,cv=5)\n",
    "score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# random - forest \n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "RF = RandomForestClassifier(n_estimators=20, random_state=0)\n",
    "RF.fit(X_train_scaled,y_train)\n",
    "\n",
    "predicted_values = RF.predict(X_scaled_test)\n",
    "\n",
    "x = metrics.accuracy_score(y_test, predicted_values)\n",
    "\n",
    "# Cross validation score (Random Forest)\n",
    "score = cross_val_score(RF,features,target,cv=5)\n",
    "score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "#xgboost\n",
    "import xgboost as xgb\n",
    "XB = xgb.XGBClassifier()\n",
    "XB.fit(X_train_scaled,y_train)\n",
    "\n",
    "predicted_values = XB.predict(X_test_scaled)\n",
    "\n",
    "x = metrics.accuracy_score(y_test, predicted_values)\n",
    "# Cross validation score (XGBoost)\n",
    "score = cross_val_score(XB,features,target,cv=5)\n",
    "score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import pickle\n",
    "# Dump the trained Naive Bayes classifier with Pickle\n",
    "RF_pkl_filename = 'RandomForest.pkl'\n",
    "# Open the file to save as pkl file\n",
    "RF_Model_pkl = open(RF_pkl_filename, 'wb')\n",
    "pickle.dump(RF, RF_Model_pkl)\n",
    "# Close the pickle instances\n",
    "RF_Model_pkl.close()"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "none",
   "dataSources": [
    {
     "datasetId": 1046158,
     "sourceId": 1760012,
     "sourceType": "datasetVersion"
    }
   ],
   "dockerImageVersionId": 31153,
   "isGpuEnabled": false,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "MLOPS-LLMOPS",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
